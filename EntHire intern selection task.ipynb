{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#                                                Objective\n",
    "You need to create an API endpoint that can accept a text and return associated sentiment with it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 airline_sentiment  \\\n",
       "0           1          positive   \n",
       "1           3          negative   \n",
       "2           4          negative   \n",
       "3           5          negative   \n",
       "4           6          positive   \n",
       "\n",
       "                                                text  \n",
       "0  @VirginAmerica plus you've added commercials t...  \n",
       "1  @VirginAmerica it's really aggressive to blast...  \n",
       "2  @VirginAmerica and it's a really big bad thing...  \n",
       "3  @VirginAmerica seriously would pay $30 a fligh...  \n",
       "4  @VirginAmerica yes, nearly every time I fly VX...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix , classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('airline_sentiment_analysis.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0          positive  @VirginAmerica plus you've added commercials t...\n",
       "1          negative  @VirginAmerica it's really aggressive to blast...\n",
       "2          negative  @VirginAmerica and it's a really big bad thing...\n",
       "3          negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "4          positive  @VirginAmerica yes, nearly every time I fly VX..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[:,df.columns!=\"Unnamed: 0\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment    0\n",
       "text                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for null values\n",
    "df.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for class imbalance\n",
    "df['airline_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import re\n",
    "\n",
    "# remove some stopwords to capture negation in n-grams if possible\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "stop_words.remove('no')\n",
    "stop_words.remove('not')\n",
    "stop_words.remove('but')\n",
    "\n",
    "\n",
    "\n",
    "def simple_text_preprocessor(document): \n",
    "    # lower case\n",
    "    document = str(document).lower()\n",
    "    \n",
    "    \n",
    "    # remove unnecessary characters\n",
    "    document = re.sub(r'[^a-zA-Z]',r' ', document)\n",
    "    document = re.sub(r'nbsp', r'', document)\n",
    "    document = re.sub(' +', ' ', document)\n",
    "    \n",
    "\n",
    "    \n",
    "    # stopwords removal\n",
    "    document = ' '.join([word for word in document.split() if word not in stop_words])\n",
    "    \n",
    "    return document\n",
    "\n",
    "stp = np.vectorize(simple_text_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica plus added commercials experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really aggressive blast obnoxiou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica seriously would pay flight seats...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica yes nearly every time fly vx ear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0          positive  virginamerica plus added commercials experienc...\n",
       "1          negative  virginamerica really aggressive blast obnoxiou...\n",
       "2          negative                 virginamerica really big bad thing\n",
       "3          negative  virginamerica seriously would pay flight seats...\n",
       "4          positive  virginamerica yes nearly every time fly vx ear..."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = stp(df['text'].values)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    virginamerica plus added commercials experienc...\n",
       "1    virginamerica really aggressive blast obnoxiou...\n",
       "2                   virginamerica really big bad thing\n",
       "3    virginamerica seriously would pay flight seats...\n",
       "4    virginamerica yes nearly every time fly vx ear...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = df['text']\n",
    "y = df['airline_sentiment']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({'positive': 1800, 'negative': 6855}),\n",
       " Counter({'negative': 2323, 'positive': 563}))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "Counter(y_train), Counter(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 model using linear svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('md', LinearSVC())])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "model = LinearSVC()\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('md',model),])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.97      0.96      2323\n",
      "    positive       0.86      0.75      0.80       563\n",
      "\n",
      "    accuracy                           0.93      2886\n",
      "   macro avg       0.90      0.86      0.88      2886\n",
      "weighted avg       0.92      0.93      0.93      2886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = text_clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 model LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('md', LogisticRegression())])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('md',model),])\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.98      0.95      2323\n",
      "    positive       0.90      0.60      0.72       563\n",
      "\n",
      "    accuracy                           0.91      2886\n",
      "   macro avg       0.90      0.79      0.83      2886\n",
      "weighted avg       0.91      0.91      0.90      2886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = text_clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.90      0.91      2323\n",
      "    positive       0.62      0.63      0.62       563\n",
      "\n",
      "    accuracy                           0.85      2886\n",
      "   macro avg       0.76      0.77      0.77      2886\n",
      "weighted avg       0.85      0.85      0.85      2886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('md',model),])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.95      0.93      2323\n",
      "    positive       0.75      0.62      0.68       563\n",
      "\n",
      "    accuracy                           0.89      2886\n",
      "   macro avg       0.83      0.79      0.81      2886\n",
      "weighted avg       0.88      0.89      0.88      2886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "clftree = tree.DecisionTreeClassifier()\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "model = BaggingClassifier(base_estimator=clftree, n_estimators=100,\n",
    "                            bootstrap=True, n_jobs=-1,\n",
    "                            random_state=42)\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('md',model),])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.98      0.94      2323\n",
      "    positive       0.89      0.61      0.72       563\n",
      "\n",
      "    accuracy                           0.91      2886\n",
      "   macro avg       0.90      0.79      0.83      2886\n",
      "weighted avg       0.91      0.91      0.90      2886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "model= RandomForestClassifier(n_estimators=250,random_state=42)\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('md',model),])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.92      0.98      0.95      2323\n",
      "    positive       0.91      0.64      0.75       563\n",
      "\n",
      "    accuracy                           0.92      2886\n",
      "   macro avg       0.91      0.81      0.85      2886\n",
      "weighted avg       0.92      0.92      0.91      2886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "model = SVC()\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('md',model),])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.93      0.93      2323\n",
      "    positive       0.69      0.69      0.69       563\n",
      "\n",
      "    accuracy                           0.88      2886\n",
      "   macro avg       0.81      0.81      0.81      2886\n",
      "weighted avg       0.88      0.88      0.88      2886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('md',model),])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.98      0.93      2323\n",
      "    positive       0.85      0.51      0.64       563\n",
      "\n",
      "    accuracy                           0.89      2886\n",
      "   macro avg       0.87      0.74      0.79      2886\n",
      "weighted avg       0.88      0.89      0.88      2886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('md',model),])\n",
    "text_clf.fit(X_train, y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.97      0.96      2323\n",
      "    positive       0.87      0.74      0.80       563\n",
      "\n",
      "    accuracy                           0.93      2886\n",
      "   macro avg       0.90      0.86      0.88      2886\n",
      "weighted avg       0.93      0.93      0.93      2886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier()\n",
    "\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('md',model),])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear svc is best among all with highest accuracy and f1 score among all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let resample train data to overcome class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# concatenate training data back together\n",
    "train_data = pd.concat([X_train, y_train], axis = 1)\n",
    "\n",
    "# separate minority and majority class\n",
    "positive_sentiment_data = train_data[train_data.airline_sentiment=='positive']\n",
    "negative_sentiment_data = train_data[train_data.airline_sentiment=='negative']\n",
    "\n",
    "# Unsample minority; we are oversampling the minority class to match the number of majority classs\n",
    "positive_upsampled = resample(positive_sentiment_data,\n",
    "                           replace = True, # Sample with replacement\n",
    "                           n_samples = len(negative_sentiment_data), # Match number in majority class\n",
    "                           random_state=27)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10146</th>\n",
       "      <td>americanair told could refund cost original re...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9803</th>\n",
       "      <td>americanair everyone else outstanding</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>united ua denver austin still ground</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9482</th>\n",
       "      <td>americanair issue lack consideration announcem...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4161</th>\n",
       "      <td>southwestair hold min trying rebook flight can...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text airline_sentiment\n",
       "10146  americanair told could refund cost original re...          negative\n",
       "9803               americanair everyone else outstanding          negative\n",
       "1103                united ua denver austin still ground          negative\n",
       "9482   americanair issue lack consideration announcem...          negative\n",
       "4161   southwestair hold min trying rebook flight can...          negative"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# combine majority and upsampled minority\n",
    "upsampled = pd.concat([negative_sentiment_data, positive_upsampled])\n",
    "upsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    6855\n",
       "positive    6855\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's check the classes count\n",
    "upsampled.airline_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = upsampled['text']\n",
    "Y_new = upsampled['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.95      0.94      0.95      2323\n",
      "    positive       0.78      0.81      0.79       563\n",
      "\n",
      "    accuracy                           0.92      2886\n",
      "   macro avg       0.87      0.88      0.87      2886\n",
      "weighted avg       0.92      0.92      0.92      2886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# now lets train again using linear svc\n",
    "model = LinearSVC()\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('md',model),])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_new, Y_new)\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.96      0.95      2323\n",
      "    positive       0.83      0.74      0.78       563\n",
      "\n",
      "    accuracy                           0.92      2886\n",
      "   macro avg       0.88      0.85      0.87      2886\n",
      "weighted avg       0.92      0.92      0.92      2886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# on oversampling it seem that model lead to overfit which result slightly bad result so let revert back to original model\n",
    "# now lets train again using linear svc with changing parameter\n",
    "model = LinearSVC(penalty='l1',dual = False)\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('md',model),])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.97      0.96      2323\n",
      "    positive       0.86      0.75      0.80       563\n",
      "\n",
      "    accuracy                           0.93      2886\n",
      "   macro avg       0.90      0.86      0.88      2886\n",
      "weighted avg       0.92      0.93      0.93      2886\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Coming backk to best model\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('L_svc', LinearSVC()),])\n",
    "\n",
    "# Feed the training data through the pipeline\n",
    "text_clf.fit(X_train, y_train)\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['job_lib_nlp_model.pk']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "  \n",
    "joblib_file = \"job_lib_nlp_model.pk\"\n",
    "joblib.dump(text_clf,joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp_course]",
   "language": "python",
   "name": "conda-env-nlp_course-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
